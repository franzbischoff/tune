<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="What are acquisition functions? Learn how are they used to guide the 
exploration of the parameter space during Bayesian optimization.
">
<title>Acquisition functions • tune</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><link href="../deps/Source_Sans_Pro-0.4.7/font.css" rel="stylesheet">
<link href="../deps/Source_Code_Pro-0.4.7/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Acquisition functions">
<meta property="og:description" content="What are acquisition functions? Learn how are they used to guide the 
exploration of the parameter space during Bayesian optimization.
">
<meta property="og:image" content="https://tune.tidymodels.org/logo.png">
<meta name="robots" content="noindex">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><script defer data-domain="tune.tidymodels.org,all.tidymodels.org" src="https://plausible.io/js/plausible.js"></script>
</head>
<body>
    <a href="#container" class="visually-hidden-focusable">Skip to content</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-none"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">tune</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">1.1.1.9001</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/tune.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/acquisition_functions.html">Acquisition functions</a>
    <a class="dropdown-item" href="../articles/extras/optimizations.html">Optimizations and parallel processing</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-learn-more">Learn more</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-learn-more">
    <a class="external-link dropdown-item" href="https://www.tidymodels.org/learn/work/tune-svm/">Grid search</a>
    <a class="external-link dropdown-item" href="https://www.tidymodels.org/learn/work/bayes-opt/">Bayesian optimization of classification model</a>
    <a class="external-link dropdown-item" href="https://www.tidymodels.org/learn/work/tune-text/">Tuning text models</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/tidymodels/tune/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article" id="container">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Acquisition functions</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/tidymodels/tune/blob/HEAD/vignettes/acquisition_functions.Rmd" class="external-link"><code>vignettes/acquisition_functions.Rmd</code></a></small>
      <div class="d-none name"><code>acquisition_functions.Rmd</code></div>
    </div>

    
    
<p><em>Acquisition functions</em> are mathematical techniques that guide
how the parameter space should be explored during Bayesian optimization.
They use the predicted mean and predicted variance generated by the
Gaussian process model. For a set of such predictions on a set of
candidate parameter sets, an acquisition functions combines the means
and variances into a criterion that will direct the search.</p>
<p>The variance term that is generated by the Gaussian process model
usually reflects the spatial aspects of the data. Candidate sets with
high variance are not near any existing parameter values (i.e. those
that have observed performance estimates). The predicted variance is
very close to zero at or very near to an existing result.</p>
<p>There is usually a trade-off between two strategies:</p>
<ul>
<li><p><em>exploitation</em> focuses on results in the vicinity of the
current best results by penalizing for higher variance values.</p></li>
<li><p><em>exploration</em> pushes the search towards unexplored
regions.</p></li>
</ul>
<p>The acquisition functions themselves have quasi-tuning parameters
that are usually trade-offs between exploitation and exploration. For
example, if the performance measure being used should be maximized
(i.e. accuracy, the area under the ROC curve, etc), then one acquisition
function would be a lower confidence bound <span class="math inline">\(L
= \mu - C \times \sigma\)</span>. The multiplier <span class="math inline">\(C\)</span> would be used to penalize based on the
predicted standard error (<span class="math inline">\(\sigma\)</span>)
of different parameter combinations. Note that the acquisition function
is not the performance measure, but a function of what metric is used to
evaluate the model.</p>
<p>One of the most common acquisition functions is the <em>expected
improvement</em>. Based on basic probability theory, this can be
computed relative to the current estimate of the optimal performance.
Suppose that our performance metric should be maximized (e.g. accuracy,
area under the ROC curve, etc). For any tuning parameter combination
<span class="math inline">\(\theta\)</span>, we have the predicted mean
and standard error of that metric (call those <span class="math inline">\(\mu(\theta)\)</span> and <span class="math inline">\(\sigma(\theta)\)</span>). From previous data, the
best (mean) performance value was <span class="math inline">\(m_{opt}\)</span>). The expected improvement is
determined using:</p>
<p><span class="math display">\[
\begin{align}
EI(\theta; m_{opt}) &amp;= \delta(\theta)
\Phi\left(\frac{\delta(\theta)}{\sigma(\theta)}\right) + \sigma(\theta)
\phi\left(\frac{\delta(\theta)}{\sigma(\theta)}\right) \notag \\
&amp;\text{where} \notag \\
\delta(\theta) &amp;= \mu(\theta) - m_{opt} \notag
\end{align}
\]</span></p>
<p>The function <span class="math inline">\(\Phi(\cdot)\)</span> is the
cumulative standard normal and <span class="math inline">\(\phi(\cdot)\)</span> is the standard normal
density.</p>
<p>The value <span class="math inline">\(\delta(\theta)\)</span>
measures how close we are (on average) to the current best performance
value<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;This equation treats &lt;span class="math inline"&gt;\(m_{opt}\)&lt;/span&gt; as if it has no variation and no
correlation to &lt;span class="math inline"&gt;\(\mu(\theta)\)&lt;/span&gt;. Neither
of those are true so consider this a “first order approximation” instead
of a highly accurate estimate.&lt;/p&gt;'><sup>1</sup></a>.
When new candidate tuning parameters are needed, the space of <span class="math inline">\(\theta\)</span> is searched for the value that
maximizes the expected improvement.</p>
<p>Suppose a single parameter were being optimized and that parameter
was represented using a log10 transformation. Using resampling, suppose
the accuracy results for three points were evaluated:</p>
<p><img src="figures/initial.svg"><!-- --></p>
<p>In the first iteration of Bayesian optimization, these three data
points are given to the Gaussian process model to produce predictions
across a wider range of values. The fitted curve (i.e. <span class="math inline">\(\mu(\theta)\)</span>) is shown on the top panel
below, along with approximate 95% credible intervals <span class="math inline">\(\mu(\theta) \pm 1.96 \sigma(\theta)\)</span>:</p>
<p><img src="figures/iter_1.svg"><!-- --></p>
<p>Notice that the interval width is large in regions far from observed
data points.</p>
<p>The bottom panel shows the expected improvement across the range of
candidate values. Of the observed points, the expected improvement near
the middle point has the largest improvement. This is because the first
term in the equation above (with the <span class="math inline">\(\delta(\theta)\)</span> coefficient) is very large
while the second term (with the coefficient <span class="math inline">\(\sigma(\theta)\)</span> is virtually zero. This
focus on the mean portion will keep the search mostly in the region of
the best performance.</p>
<p>Using these results, the parameter value with the largest improvement
is then evaluated using cross-validation. The GP model is then updated
and a new parameter is chosen and so on.</p>
<p>The results at iteration 20 were:</p>
<p><img src="figures/iter_20.svg"><!-- --></p>
<p>The points shown on the graph indicate that there is a region in the
neighborhood of 0.01 that appears to produce the best results, and that
the expected improvement function has driven the optimization to focus
on this region.</p>
<p>When using expected improvement, the primary method for compromising
between exploitation and exploration is the use of a “trade-off” value.
This value is the amount of performance (in the original units) that can
be sacrificed when computing the improvement. This has the effect of
down-playing the contribution of the mean effect in the computations.
For a trade-off value <span class="math inline">\(\tau\)</span>, the
equation above uses:</p>
<p><span class="math display">\[
\delta(\theta) = \mu(\theta) - m_{opt} - \tau
\]</span></p>
<p>Suppose that we were willing to trade-off <span class="math inline">\(\tau = 0.05\)</span>% of the predicted accuracy
during the search. Using the same three initial results, the procedure
would end up in the same general location but would have explored more
values across the total range:</p>
<p><img src="figures/trade_off_20.svg"><!-- --></p>
<p>There are two main strategies for <em>dynamic trade-offs</em> during
the optimization:</p>
<ul>
<li><p>Use a function to specify the parameter(s) for the acquisition
functions. For expected improvement, this can be done using
<code>exp_improve(trade_off = foo())</code>. <code>foo()</code> should
be a function whose first parameter is the current iteration number.
When <code>tune</code> invokes this function, only the first argument is
used. A good strategy might be to set <code>trade_off</code> to some
non-zero value at the start of the search and incrementally approach
zero after a reasonable period.</p></li>
<li><p><code><a href="../reference/control_bayes.html">control_bayes()</a></code> has an option for doing an
additional <em>uncertainty sample</em> when no improvements have been
found. This is a technique from the active learning literature where new
data points are sampled that most help the model. In this case, the
candidate points are scored only on variance and a candidate is chosen
from a set of the <em>most</em> variable design points. This may find a
location in the parameter space to help the optimization make
improvements.</p></li>
</ul>
  </main>
</div>



   </div>
  <footer><div class="container">
  <div class="pkgdown-footer-left">
  <p></p>
<p>Developed by <a href="https://github.com/topepo" class="external-link">Max Kuhn</a>, <a href="https://www.posit.co" class="external-link"><img src="https://www.tidyverse.org/posit-logo.svg" alt="Posit" height="16" style="margin-bottom: 3px;"></a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

  </div></footer>
</body>
</html>
